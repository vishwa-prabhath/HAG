{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcac20b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model (GPT2 for generation)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# Load emotion classifier\n",
    "classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "def dynamic_hyperparameter_selector(task_type: str):\n",
    "    if task_type == \"factual\":\n",
    "        return {\"temperature\": 0.3, \"top_p\": 0.85, \"top_k\": 40, \"repetition_penalty\": 1.2}\n",
    "    elif task_type == \"creative\":\n",
    "        return {\"temperature\": 0.95, \"top_p\": 0.95, \"top_k\": 100, \"repetition_penalty\": 1.0}\n",
    "    elif task_type == \"emotional\":\n",
    "        return {\"temperature\": 0.7, \"top_p\": 0.9, \"top_k\": 50, \"repetition_penalty\": 1.1}\n",
    "    else:\n",
    "        return {\"temperature\": 0.5, \"top_p\": 0.85, \"top_k\": 60, \"repetition_penalty\": 1.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8658d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def infer_task_type(prompt):\n",
    "    result = classifier(prompt)[0]['label']\n",
    "    if result in [\"joy\", \"surprise\"]:\n",
    "        return \"creative\"\n",
    "    elif result in [\"sadness\", \"fear\", \"love\"]:\n",
    "        return \"emotional\"\n",
    "    else:\n",
    "        return \"factual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d794d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt: str):\n",
    "    task_type = infer_task_type(prompt)\n",
    "    hyperparams = dynamic_hyperparameter_selector(task_type)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        temperature=hyperparams[\"temperature\"],\n",
    "        top_p=hyperparams[\"top_p\"],\n",
    "        top_k=hyperparams[\"top_k\"],\n",
    "        repetition_penalty=hyperparams[\"repetition_penalty\"],\n",
    "        max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True), task_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "    response, task_type = generate_response(prompt)\n",
    "    print(f\"\\n[Task Type: {task_type}]\\n\")\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
