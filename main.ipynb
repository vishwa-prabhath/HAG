{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcac20b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model (GPT2 for generation)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# Load emotion classifier\n",
    "classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "def dynamic_hyperparameter_selector(task_type: str):\n",
    "    if task_type == \"factual\":\n",
    "        return {\"temperature\": 0.3, \"top_p\": 0.85, \"top_k\": 40, \"repetition_penalty\": 1.2}\n",
    "    elif task_type == \"creative\":\n",
    "        return {\"temperature\": 0.95, \"top_p\": 0.95, \"top_k\": 100, \"repetition_penalty\": 1.0}\n",
    "    elif task_type == \"emotional\":\n",
    "        return {\"temperature\": 0.7, \"top_p\": 0.9, \"top_k\": 50, \"repetition_penalty\": 1.1}\n",
    "    else:\n",
    "        return {\"temperature\": 0.5, \"top_p\": 0.85, \"top_k\": 60, \"repetition_penalty\": 1.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8658d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def infer_task_type(prompt):\n",
    "    result = classifier(prompt)[0]['label']\n",
    "    if result in [\"joy\", \"surprise\"]:\n",
    "        return \"creative\"\n",
    "    elif result in [\"sadness\", \"fear\", \"love\"]:\n",
    "        return \"emotional\"\n",
    "    else:\n",
    "        return \"factual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d794d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt: str):\n",
    "    task_type = infer_task_type(prompt)\n",
    "    hyperparams = dynamic_hyperparameter_selector(task_type)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        temperature=hyperparams[\"temperature\"],\n",
    "        top_p=hyperparams[\"top_p\"],\n",
    "        top_k=hyperparams[\"top_k\"],\n",
    "        repetition_penalty=hyperparams[\"repetition_penalty\"],\n",
    "        max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True), task_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "    response, task_type = generate_response(prompt)\n",
    "    print(f\"\\n[Task Type: {task_type}]\\n\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947aae5",
   "metadata": {},
   "source": [
    "# Anton 06/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6cb9378",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, pipeline\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import collections \n",
    "from chromaDB_logger import store_to_chroma\n",
    "\n",
    "# --- Configuration ---\n",
    "MEMORY_SIZE = 3 # How many past sets of hyperparameters to remember\n",
    "CURRENT_TASK_WEIGHT = 0.7 # How much the current task type influences the new hyperparameters\n",
    "PREVIOUS_MEMORY_WEIGHT = 0.3 # How much the average of past hyperparameters influences the new ones\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "\n",
    "# Initialize hyperparameter memory using a deque for fixed-size history\n",
    "hyperparameter_memory = collections.deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Vishwa's Function\n",
    "def dynamic_hyperparameter_selector(task_type: str):\n",
    "    \"\"\"\n",
    "    Selects a base set of hyperparameters based on the task type.\n",
    "    These are the 'target' hyperparameters for the current prompt.\n",
    "    \"\"\"\n",
    "    if task_type == \"factual\":\n",
    "        return {\"temperature\": 0.3, \"top_p\": 0.85, \"top_k\": 40, \"repetition_penalty\": 1.2}\n",
    "    elif task_type == \"creative\":\n",
    "        return {\"temperature\": 0.95, \"top_p\": 0.95, \"top_k\": 100, \"repetition_penalty\": 1.0}\n",
    "    elif task_type == \"emotional\":\n",
    "        return {\"temperature\": 0.7, \"top_p\": 0.9, \"top_k\": 50, \"repetition_penalty\": 1.1}\n",
    "    else: \n",
    "        return {\"temperature\": 0.5, \"top_p\": 0.85, \"top_k\": 60, \"repetition_penalty\": 1.1}\n",
    "\n",
    "def infer_task_type(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Infers the task type based on the prompt's emotional content.\n",
    "    \"\"\"\n",
    "    result = classifier(prompt)[0]['label']\n",
    "    if result in [\"joy\", \"surprise\"]:\n",
    "        return \"creative\"\n",
    "    elif result in [\"sadness\", \"fear\", \"love\"]:\n",
    "        return \"emotional\"\n",
    "    else:\n",
    "        return \"factual\"\n",
    "\n",
    "# Hyper Parameter Smoothing function -- Anton\n",
    "def smooth_hyperparameters(current_task_params: dict, memory: collections.deque) -> dict:\n",
    "    \n",
    "    if not memory:\n",
    "        print(\"Memory is empty. Using direct task parameters.\")\n",
    "        return current_task_params\n",
    "\n",
    "    avg_memory_params = {\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 0.0,\n",
    "        \"top_k\": 0.0,\n",
    "        \"repetition_penalty\": 0.0\n",
    "    }\n",
    "\n",
    "    for past_params in memory:\n",
    "        for key in avg_memory_params:\n",
    "            avg_memory_params[key] += past_params[key]\n",
    "\n",
    "    for key in avg_memory_params:\n",
    "        avg_memory_params[key] /= len(memory)\n",
    "\n",
    "    # Apply the weighted average to smooth the parameters\n",
    "    smoothed_params = {\n",
    "        \"temperature\": (current_task_params[\"temperature\"] * CURRENT_TASK_WEIGHT) +\n",
    "                       (avg_memory_params[\"temperature\"] * PREVIOUS_MEMORY_WEIGHT),\n",
    "        \"top_p\": (current_task_params[\"top_p\"] * CURRENT_TASK_WEIGHT) +\n",
    "                 (avg_memory_params[\"top_p\"] * PREVIOUS_MEMORY_WEIGHT),\n",
    "        \"top_k\": int((current_task_params[\"top_k\"] * CURRENT_TASK_WEIGHT) +\n",
    "                     (avg_memory_params[\"top_k\"] * PREVIOUS_MEMORY_WEIGHT)), \n",
    "        \"repetition_penalty\": (current_task_params[\"repetition_penalty\"] * CURRENT_TASK_WEIGHT) +\n",
    "                              (avg_memory_params[\"repetition_penalty\"] * PREVIOUS_MEMORY_WEIGHT)\n",
    "    }\n",
    "\n",
    "    # Add constraints to ensure parameters stay within reasonable bounds\n",
    "    smoothed_params[\"temperature\"] = max(0.1, min(smoothed_params[\"temperature\"], 1.0))\n",
    "    smoothed_params[\"top_p\"] = max(0.1, min(smoothed_params[\"top_p\"], 1.0))\n",
    "    smoothed_params[\"top_k\"] = max(1, min(smoothed_params[\"top_k\"], tokenizer.vocab_size - 1)) # top_k can't exceed vocab size\n",
    "    smoothed_params[\"repetition_penalty\"] = max(1.0, min(smoothed_params[\"repetition_penalty\"], 2.0)) # A common range\n",
    "\n",
    "    return smoothed_params\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    global hyperparameter_memory # Declare intent to modify the global variable\n",
    "\n",
    "    task_type = infer_task_type(prompt)\n",
    "    \n",
    "    # Get the target hyperparameters for the current task\n",
    "    current_task_target_params = dynamic_hyperparameter_selector(task_type)\n",
    "\n",
    "    # Smooth these target parameters with the memory\n",
    "    final_hyperparams = smooth_hyperparameters(current_task_target_params, hyperparameter_memory)\n",
    "\n",
    "    hyperparameter_memory.append(final_hyperparams)\n",
    "\n",
    "    print(f\"\\nDebug Info:\")\n",
    "    print(f\"  Inferred Task Type: {task_type}\")\n",
    "    print(f\"  Current Task Target Hyperparams: {current_task_target_params}\")\n",
    "    print(f\"  Smoothed/Final Hyperparams Used: {final_hyperparams}\")\n",
    "    print(f\"  Current Memory State ({len(hyperparameter_memory)} entries): {list(hyperparameter_memory)}\")\n",
    "\n",
    "    # updating the model to bypass the vague response generation Issue - Anton\n",
    "    new_prompt = \"Assume that you are an Ai agent Answering a question\\n Question: \\\" \"+prompt +\"\\\"\"\n",
    "    inputs = tokenizer(new_prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        temperature=final_hyperparams[\"temperature\"],\n",
    "        top_p=final_hyperparams[\"top_p\"],\n",
    "        top_k=final_hyperparams[\"top_k\"],\n",
    "        repetition_penalty=final_hyperparams[\"repetition_penalty\"],\n",
    "        max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True), task_type\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the Dynamic Hyperparameter AI Chatbot!\")\n",
    "    print(\"Type 'quit' or 'exit' to end the conversation.\")\n",
    "\n",
    "    # starting with a 'neutral' base:\n",
    "    initial_params = {\"temperature\": 0.5, \"top_p\": 0.85, \"top_k\": 60, \"repetition_penalty\": 1.1}\n",
    "    hyperparameter_memory.append(initial_params)\n",
    "    print(f\"Initial Memory State: {list(hyperparameter_memory)}\")\n",
    "\n",
    "\n",
    "    while True:\n",
    "        prompt = input(\"\\nEnter your prompt: \")\n",
    "        if prompt.lower() in ['quit', 'exit']:\n",
    "            print(\"Exiting chat. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response, task_type = generate_response(prompt)\n",
    "        print(f\"\\n[Inferred Task Type: {task_type}]\")\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "        # Log this interaction in ChromaDB\n",
    "        store_to_chroma(prompt, response, task_type)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4ac55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
